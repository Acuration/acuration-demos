# -*- coding: utf-8 -*-
"""sum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zLIWwuhS5TFd2zhim-9jhRrzaN65RDGP
"""

# pip install --upgrade google-search-results
# pip install google-search-results
# !pip install serpapi

queries=["What is the market size of Enphase Energy (Headquarters: Hyderabad, India)",
"What is the competitive landscape of Enphase Energy (Headquarters: Hyderabad, India)"," What are the business models of Enphase Energy (Headquarters: Hyderabad, India)",

"What are the revenue streams of Enphase Energy (Headquarters: Hyderabad, India)","What is the pricing model of Enphase Energy (Headquarters: Hyderabad, India)",
"What is the share price of Enphase Energy (Headquarters: Hyderabad, India)","What is the profit margin of Enphase Energy (Headquarters: Hyderabad, India)",
"What is the total user base of Enphase Energy (Headquarters: Hyderabad, India)","How many paying customers do they have of Enphase Energy (Headquarters: Hyderabad, India)",

"What are the social media platforms of Enphase Energy (Headquarters: Hyderabad, India)","How many followers on social media platforms for of Enphase Energy (Headquarters: Hyderabad, India)",
"What is the funding info of Enphase Energy (Headquarters: Hyderabad, India)","What is the vision and mission of Enphase Energy (Headquarters: Hyderabad, India)","What are the key capabilities of Enphase Energy (Headquarters: Hyderabad, India) ","What is their marketing strategy of Enphase Energy (Headquarters: Hyderabad, India)",
"What is the business strategy of Enphase Energy (Headquarters: Hyderabad, India)","Does ‘company’ have any collaborations or partnerships with Enphase Energy (Headquarters: Hyderabad, India)","What is the cash flow of Enphase Energy (Headquarters: Hyderabad, India)"
]
queries

from serpapi import GoogleSearch
def SERPAPI(query):
  params = {
    "q": query,
    "hl": "en",
    "gl": "us",
    "api_key": "3f5a92720a2d9a24b21b98d9567bd0aba2c63159727a72416676b6c3928f100e"
  }
  search = GoogleSearch(params)
  results = search.get_dict()
  return results

from bs4 import BeautifulSoup
import re
def clean_html_and_extract_text(html_text):
    try:
        soup = BeautifulSoup(html_text, 'html.parser')
        text_content = soup.get_text(separator=' ', strip=True)
        # Replace multiple consecutive newlines with a single newline
        text_content = re.sub('\n+', '\n', text_content)


        result = text_content.split('\n')
        result = [x for x in result if x != '']
        return "\n".join(result)
    except Exception as e:
        print(f"Failed with parser. Error: {e}")
        return None

import requests
def fetch_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        # Check if the content is PDF
        if 'application/pdf' in response.headers.get('Content-Type', ''):
            return response.content, 'pdf'
        else:
            # return response.text, 'html'
            # Extract text from HTML content using BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text(separator='\n')  # Separate text with newlines
            return text, 'html'

    except requests.exceptions.RequestException as e:
        print(f"Failed to fetch content from {url}. Error: {e}")
        return None, None

if "related_question" in results:
  file_path = r"/content/Whereistheheadquarterslocationrelated.txt"
  with open(file_path, 'a') as file:
      for string in results['related_questions']:
          # Redirect the output to the file
          if 'question' in string:
            file.write(string['question'] + '\n')
          if 'title' in string:
            file.write(string['title'] + '\n')
          if 'snippet' in string:
            file.write(string['snippet']+ '\n')
          if 'link' in string:
            html_content,st=fetch_content(string['link'])
            html_content=clean_html_and_extract_text(html_content)
            file.write(html_content)
  print("Done writing to:", file_path)

def summarization_text(content):
  url = "https://www.semrush.com/goodcontent/api/summary-generator/generate-summary/"
  payload = {
    "text": content,
    "format": "paragraph",
    "length_penalty": 0
  }
  # Make the POST request
  response = requests.post(url, json=payload)
  # Check if the request was successful (status code 200)
  if response.status_code == 200:
    # Parse and print the response JSON
    response_data = response.json()
    return response_data["summary"]
  else:
    return 'None'

# Open a file in write mode ('w')
file_path = r"/content/question3.txt"
c=0

for query in queries:
  results=SERPAPI(query)
  with open(file_path, 'a') as file:
    file.write(query+"\n")
    try:
      if 'answer_box' in results:
          for string in results['answer_box']:
              if 'answer' in string:
                file.write("\t"+string['answer'] + '\n')
              if 'title' in string:
                file.write("\t\t"+string['title'] + '\n')
              if 'snippet' in string:
                file.write("\t\t\t"+string['snippet'] + '\n')
              if 'link' in string:
                html_content,st=fetch_content(string['link'])
                if html_content:
                  html_content=clean_html_and_extract_text(html_content)
                if html_content:
                  html_content=summarization_text(html_content)
                if html_content:
                  lines=html_content.split("\n")
                  for line in lines:
                    file.write("\t\t\t\t"+line+"\n")
    except Exception as ex:
      print("Error ocurred")



    try:
      if 'organic_results' in results:
          for string in results['organic_results']:
              # Redirect the output to the file
              if 'title' in string:
                file.write("\t"+string['title'] + '\n')
              if 'snippet' in string:
                file.write("\t\t"+string['snippet'] + '\n')
              if 'link' in string:
                html_content,st=fetch_content(string['link'])
                if html_content:
                  html_content=clean_html_and_extract_text(html_content)
                if html_content:
                  html_content=summarization_text(html_content)
                if html_content:
                  lines=html_content.split("\n")
                  for line in lines:
                    file.write("\t\t\t"+line+"\n")
    except Exception as ex:
      print("Error ocurred")



    try:
      if 'related_questions' in results:
          for string in results['related_questions']:
              # Redirect the output to the file
              if 'question' in string:
                file.write("\t"+string['question'] + '\n')
              if 'title' in string:
                file.write("\t\t"+string['title'] + '\n')
              if 'snippet' in string:
                file.write("\t\t\t"+string['snippet']+ '\n')
              if 'link' in string:
                html_content,st=fetch_content(string['link'])
                if html_content:
                  html_content=clean_html_and_extract_text(html_content)
                if html_content:
                  html_content=summarization_text(html_content)
                if html_content:
                  lines=html_content.split("\n")
                  for line in lines:
                    file.write("\t\t\t\t"+line+"\n")

    except Exception as ex:
      print("Error ocurred")
    file.write("\n\n\n")
  c+=1
  print(c)