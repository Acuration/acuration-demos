# -*- coding: utf-8 -*-
"""acuration serp api.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11XcQ2WH1vX9hmGuFgR6T3LcUIphPLaUF
"""

pip install serpapi

import serpapi
def serp(info):
  client = serpapi.Client(api_key="cce5b81c732cf1c4001f97a8a5b4e8b72e21d3eee0c26c46da402f14ca0b2f0c")
  res = client.search({
      "engine": "google",
      "q": info,
      "location": "Cardiff,Wales"
  })
  return res
  #print(res)

from google.colab import drive
drive.mount('/content/drive')

s=" of The Solar Guys company?"
questions = [
              "What is the official website"+s,
              "What is the bio"+s,
              "Where is the headquarters' location"+s,
              "What are the products and services"+s,
              "What is the unique selling point (USP)"+s,
              "What is the value proposition"+s,
              "What is the target market"+s,
              "What is the market size"+s,
              "What is the competitive landscape"+s,
              "What are the business models"+s,
              "What are the revenue streams"+s,
              "What is the pricing model"+s,
              "What is the share price"+s,
              "What is the profit margin"+s,
              "What is the total user base"+s,
              "How many paying customers do they have"+s,
              "What are the social media platforms"+s,
              "How many followers on social media platforms"+s,
              "What is the funding info"+s,
              "What is the vision and mission"+s,
              "What are the key capabilities"+s,
              "What is their marketing strategy"+s,
              "What is the business strategy"+s,
              "The Solar Guys company have any collaborations or partnerships"+s,
              "What is the cash flow"+s
]

from bs4 import BeautifulSoup
import re
import os
def clean_html_and_extract_text(html_text):
    try:
        soup = BeautifulSoup(html_text, 'html.parser')
        text_content = soup.get_text(separator=' ', strip=True)

        # Replace multiple consecutive newlines with a single newline
        text_content = re.sub('\n+', '\n', text_content)

        result = text_content.split('\n')
        result = [x for x in result if x != '']
        return "\n".join(result)
    except Exception as e:
        print(f"Failed with parser. Error: {e}")
        return None
import requests
def fetch_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        # Check if the content is PDF
        if 'application/pdf' in response.headers.get('Content-Type', ''):
            return response.content, 'pdf'
        else:
            # return response.text, 'html'
            # Extract text from HTML content using BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text(separator='\n')  # Separate text with newlines
            return text, 'html'

    except requests.exceptions.RequestException as e:
        print(f"Failed to fetch content from {url}. Error: {e}")
        return None, None

import json
import os
import hashlib
def generate_file_name(text):
    # Generate MD5 hash of the text
    md5_hash = hashlib.md5(text.encode()).hexdigest()
    return md5_hash

def summarisation(info):
  import requests

  url = "https://www.semrush.com/goodcontent/api/summary-generator/generate-summary/"

  payload = {
      "format": "paragraph",
      "length_penalty": 0,
      "text": info
  }

  headers = {
      "Content-Type": "application/json"
  }

  response = requests.post(url, json=payload, headers=headers)

  # Check if the request was successful (status code 200)
  if response.status_code == 200:
      # Print the response JSON
      data=response.json()
      return data["summary"]
  else:
      return None

output_file = '/content/drive/MyDrive/Colab Notebooks/final_the solar guys.txt'
with open(output_file, 'a', encoding='utf-8') as f:
    for ques in questions:
        file_name=generate_file_name(ques)
        filepath = f"{file_name}.txt"
        folder_path='/content/drive/MyDrive/Colab Notebooks/total_queries'
        fp=os.path.join(folder_path,filepath)
        file_dir=os.listdir(folder_path)
        #print(fp)
        try:
        # Check if the file already exists
          if filepath not in file_dir:
            #print("hello")
            res = serp(ques)
            #print(res)
            # Open the file in write mode ('w')
            serializable_data = {"result": res.data}
            with open(fp, 'w') as file:
              # Write the content to the file
              json.dump(serializable_data,file)
            #print(f"Text file '{file_path}' created successfully at '{file_path}'")
          else:
            with open(fp, 'r') as file:
              res=json.load(file)
            #print(f"Text file '{file_path}' already exists at '{file_path}'")

        except Exception as e:
          print(f"Error: {e}")

        f.write(ques + "\n")
        try:
          if 'answer_box' in res:
              answer_box_value = res['answer_box']
              if isinstance(answer_box_value, list):
                  for result in answer_box_value:
                      if isinstance(result, dict):  # Check if result is a dictionary
                          if 'answer' in result:
                              ans = result.get('answer', '')
                              f.write("\t" + ans + "\n")
                          if 'title' in result:
                              tit = result.get('title', '')
                              f.write("\t" + tit + "\n")
                          if 'snippet' in result:
                              snip = result.get('snippet', '')
                              f.write("\t" + snip + "\n")
                          if 'link' in result:
                              lin = result.get('link', '')
                              content, typ = fetch_content(lin)
                              if content is not None:
                                  information = clean_html_and_extract_text(content)
                                  if information is not None:
                                      final = summarisation(information)
                                      if final is not None:
                                          lines = final.split("\n")
                                          for line in lines:
                                              f.write("\t" + line + "\n")
                      else:
                          # Handle the case where result is not a dictionary
                          f.write("\t" + str(result) + "\n")
        except Exception as ex:
          print("Error ocurred : ",ex)
        try:
          if 'organic_results' in res:
              organic_results_value = res['organic_results']
              if isinstance(organic_results_value, list):
                  for result in organic_results_value:
                      if isinstance(result, dict):
                          if 'title' in result:
                              tit = result.get('title', '')
                              f.write("\t" + tit + "\n")
                          if 'snippet' in result:
                              snip = result.get('snippet', '')
                              f.write("\t" + snip + "\n")
                          if 'link' in result:
                              lin = result.get('link', '')
                              content, typ = fetch_content(lin)
                              if content is not None:
                                  information = clean_html_and_extract_text(content)
                                  if information is not None:
                                      final = summarisation(information)
                                      if final is not None:
                                          lines = final.split("\n")
                                          for line in lines:
                                              f.write("\t" + line + "\n")
                      else:
                          f.write("\t" + str(result) + "\n")
        except Exception as ex:
          print("Error ocurred : ",ex)

        try:
          if 'related_questions' in res:
              related_questions_value = res['related_questions']
              if isinstance(related_questions_value, list):
                  for result in related_questions_value:
                      if isinstance(result, dict):
                          if 'question' in result:
                              que = result.get('question', '')
                              f.write("\t" + que + "\n")
                          if 'snippet' in result:
                              snip = result.get('snippet', '')
                              f.write("\t\t" + snip + "\n")
                          if 'title' in result:
                              tit = result.get('title', '')
                              f.write("\t\t" + tit + "\n")
                          if 'link' in result:
                              lin = result.get('link', '')
                              content, typ = fetch_content(lin)
                              if content is not None:
                                  information = clean_html_and_extract_text(content)
                                  if information is not None:
                                      final = summarisation(information)
                                      if final is not None:
                                          lines = final.split("\n")
                                          for line in lines:
                                              f.write("\t\t" + line + "\n")

                      else:
                          f.write("\t\t" + str(result) + "\n")
        except Exception as ex:
          print("Error ocurred : ",ex)
        f.write("\n\n")

