# -*- coding: utf-8 -*-
"""Another copy of .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CYVr4zB0o3eC3Zm5axiZMk0KPkzPZC4O
"""

!pip install --upgrade google-search-results
# !pip install google-search-results
# !pip install serpapi

from posix import F_LOCK
import os
import json
# Open a file in write mode ('w')
folder_path=r"/content/Questions" # main folder(need to creat it)
file_path=r"/content/SunFuel_company.txt"
c=0
with open(file_path, 'a') as file:
  for query in queries:
    file.write(query+"\n")
    f=0
    md5_result=generate_md5_hash(query)

    file_name = f"{md5_result}.txt"

    files_in_directory = os.listdir(folder_path)


    if file_name in files_in_directory:
      # Reading dictionary from the file
      try:
        # Opening the file in read mode
        file_path_inner = os.path.join(folder_path, file_name)
        with open(file_path_inner, 'r') as f:
            results = json.load(f)
      except FileNotFoundError:
          print(f"The file '{file_name}' was not found.")
      f=1


    else:
      results=SERPAPI(query)
    try:
      if 'answer_box' in results:
          for string in results['answer_box']:
              if 'answer' in string:
                file.write("\t"+string['answer'] + '\n')
              if 'title' in string:
                file.write("\t"+string['title'] + '\n')
              if 'snippet' in string:
                file.write("\t"+string['snippet'] + '\n')
              if 'link' in string:
                html_content,st=fetch_content(string['link'])
                if html_content:
                  html_content=clean_html_and_extract_text(html_content)
                if html_content:
                  html_content=summarization_text(html_content)
                if html_content:
                  lines=html_content.split("\n")
                  for line in lines:
                    file.write("\t"+line+"\n")
    except Exception as ex:
      print("Error ocurred in answer box",ex)

    try:
      if 'organic_results' in results:
          for string in results['organic_results']:
              # Redirect the output to the file
              if 'title' in string:
                file.write("\t"+string['title'] + '\n')
              if 'snippet' in string:
                file.write("\t"+string['snippet'] + '\n')
              if 'link' in string:
                html_content,st=fetch_content(string['link'])
                if html_content:
                  html_content=clean_html_and_extract_text(html_content)
                if html_content:
                  html_content=summarization_text(html_content)
                if html_content:
                  lines=html_content.split("\n")
                  for line in lines:
                    file.write("\t"+line+"\n")
    except Exception as ex:
      print("Error ocurred in organic results",ex)

    try:
      if 'related_questions' in results:
          for string in results['related_questions']:
              # Redirect the output to the file
              if 'question' in string:
                file.write("\t"+string['question'] + '\n')
              if 'title' in string:
                file.write("\t\t"+string['title'] + '\n')
              if 'snippet' in string:
                file.write("\t\t"+string['snippet']+ '\n')
              if 'link' in string:
                html_content,st=fetch_content(string['link'])
                if html_content:
                  html_content=clean_html_and_extract_text(html_content)
                if html_content:
                  html_content=summarization_text(html_content)
                if html_content:
                  lines=html_content.split("\n")
                  for line in lines:
                    file.write("\t\t"+line+"\n")

    except Exception as ex:
      print("Error ocurred")
    if f==0:
      res=add_file_to_folder(file_name,folder_path,results)
      print(res)
    file.write("\n\n\n")

    c+=1
    print(c)



from serpapi import GoogleSearch
def SERPAPI(query):
  params = {
    "q": query,
    "hl": "en",
    "gl": "us",
    "api_key": "3f5a92720a2d9a24b21b98d9567bd0aba2c63159727a72416676b6c3928f100e"
  }
  search = GoogleSearch(params)
  results = search.get_dict()
  return results

queries=[
    "What is the official website of SunFuel Energy (Headquarters: Hyderabad, India)",
    "What is the bio of SunFuel Energy (Headquarters: Hyderabad, India)",
    "Where is the headquarters' location of SunFuel Energy",
    "What are the products and services of SunFuel Energy (Headquarters: Hyderabad, India)",
    "What is the unique selling point (USP) of SunFuel Energy",
    "What is the value proposition of SunFuel Energy (Headquarters: Hyderabad, India)",
    "What is the target market of SunFuel Energy (Headquarters: Hyderabad, India)",
    "What is the market size of SunFuel Energy (Headquarters: Hyderabad, India)",
"What is the competitive landscape of SunFuel Energy (Headquarters: Hyderabad, India)"," What are the business models of SunFuel Energy (Headquarters: Hyderabad, India)",

"What are the revenue streams of SunFuel Energy (Headquarters: Hyderabad, India)","What is the pricing model of SunFuel Energy (Headquarters: Hyderabad, India)",
"What is the share price of SunFuel Energy (Headquarters: Hyderabad, India)","What is the profit margin of SunFuel Energy (Headquarters: Hyderabad, India)",
"What is the total user base of SunFuel Energy (Headquarters: Hyderabad, India)","How many paying customers do they have of SunFuel Energy (Headquarters: Hyderabad, India)",

"What are the social media platforms of SunFuel Energy (Headquarters: Hyderabad, India)","How many followers on social media platforms for of SunFuel Energy (Headquarters: Hyderabad, India)",
"What is the funding info of SunFuel Energy (Headquarters: Hyderabad, India)","What is the vision and mission of SunFuel Energy (Headquarters: Hyderabad, India)","What are the key capabilities of SunFuel Energy (Headquarters: Hyderabad, India) ","What is their marketing strategy of SunFuel Energy (Headquarters: Hyderabad, India)",
"What is the business strategy of SunFuel Energy (Headquarters: Hyderabad, India)","Does ‘company’ have any collaborations or partnerships with SunFuel Energy (Headquarters: Hyderabad, India)","What is the cash flow of SunFuel Energy (Headquarters: Hyderabad, India)"
]
queries

from bs4 import BeautifulSoup
import re
def clean_html_and_extract_text(html_text):
    try:
        soup = BeautifulSoup(html_text, 'html.parser')
        text_content = soup.get_text(separator=' ', strip=True)
        # Replace multiple consecutive newlines with a single newline
        text_content = re.sub('\n+', '\n', text_content)


        result = text_content.split('\n')
        result = [x for x in result if x != '']
        return "\n".join(result)
    except Exception as e:
        print(f"Failed with parser. Error: {e}")
        return None

import requests
def fetch_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        # Check if the content is PDF
        if 'application/pdf' in response.headers.get('Content-Type', ''):
            return response.content, 'pdf'
        else:
            # return response.text, 'html'
            # Extract text from HTML content using BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text(separator='\n')  # Separate text with newlines
            return text, 'html'

    except requests.exceptions.RequestException as e:
        print(f"Failed to fetch content from {url}. Error: {e}")
        return None, None

def summarization_text(content):
  url = "https://www.semrush.com/goodcontent/api/summary-generator/generate-summary/"
  payload = {
    "text": content,
    "format": "paragraph",
    "length_penalty": 0
  }
  # Make the POST request
  response = requests.post(url, json=payload)
  # Check if the request was successful (status code 200)
  if response.status_code == 200:
    # Parse and print the response JSON
    response_data = response.json()
    return response_data["summary"]
  else:
    return 'None'

c="Once upon a time in the verdant valleys of a land known as Veridiania, where emerald-green meadows stretched beyond the horizon and the scent of wildflowers perfumed the air, there existed a quaint village named Meadowbrook. At the heart of this tranquil haven lived a young girl named Aurora, whose existence was an enchanting blend of curiosity and a boundless love for stories. Aurora possessed an insatiable thirst for knowledge, spending her days wandering through the lush forests that bordered the village, gathering tales from ancient trees and learning the secrets whispered by the babbling brooks that meandered through the woods."
summarization_text(c)

import hashlib
def generate_md5_hash(input_string):
    md5_hash = hashlib.md5()
    md5_hash.update(input_string.encode('utf-8'))
    return md5_hash.hexdigest()

import shutil
import os
import json
def add_file_to_folder(file_name, folder_path, content):
    # Check if the folder exists, if not, create it

    file_path = os.path.join(folder_path, file_name)
    # Check if the file already exists, if not, create it
    with open(file_path, 'w') as file:
        json.dump(content,file)
    return f"File '{file_name}' created and added to the folder '{folder_path}'"

import os
fn="sample.txt"
file_path= os.path.join(os.getcwd(), fn)
folder_path=r"/content/Questions"
result = add_file_to_folder(file_path, folder_path,{})
print(result)

