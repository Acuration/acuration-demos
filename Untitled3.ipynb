{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBh5e7TlmYK2y9QJNJGy6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshithbollam/acuration-demos/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbLG8JzzJwjx",
        "outputId": "62bf4660-08b5-443d-f069-e1c42f8dddf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fetent (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for fetent\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "sitemap_path='resgroup__sitemap.xml'\n",
        "\n",
        "\n",
        "\n",
        "def parse_sitemap(sitemap_path):\n",
        "  #parsing\n",
        "  tree = ET.parse(sitemap_path)\n",
        "  root = tree.getroot()\n",
        "  # Define the XML namespace\n",
        "  ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "  # Find all URL elements in the sitemap\n",
        "  url_elements = root.findall('.//ns:url/ns:loc', namespaces=ns)\n",
        "  # Extract URLs from the elements\n",
        "  url_list = [url_element.text for url_element in url_elements]\n",
        "  return url_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html_and_extract_text(html_text):\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "    # Extract text content\n",
        "    text_content = soup.get_text(separator=' ')\n",
        "    # Remove special characters and extra whitespaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text_content).strip()\n",
        "    return cleaned_text\n",
        "html_text = \"\"\"\n",
        "<html>\n",
        "  <body>\n",
        "    <p>This is an example <b>HTML</b> document.</p>\n",
        "    <p>It contains <a href=\"#\">links</a>, <em>emphasis</em>, and other <strong>markup</strong>.</p>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "clean_html_and_extract_text(html_text)\n",
        "import requests\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "def fetch_html_content(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    # Use the requests library to fetch the HTML content from the URL\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        # Handle the case when the request was not successful\n",
        "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "def save_to_file(content, folder_path, filename):\n",
        "    # Create the folder if it doesn't exist\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    # Write the content to the file\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "def crawl_url(url, folder_path):\n",
        "    # Fetch the website content using requests\n",
        "    html_content = fetch_html_content(url)\n",
        "    if html_content is not None:\n",
        "        # Clean the HTML content using the clean_html_and_extract_text function\n",
        "        extracted_text = clean_html_and_extract_text(html_content)\n",
        "        # Generate a filename based on the crawled URL with underscores\n",
        "        parsed_url = urlparse(url)\n",
        "        filename = \"crawled_\" + parsed_url.netloc + parsed_url.path.replace(\"/\", \"_\") + \"_\"\n",
        "        # Store the HTML content and extracted text in separate files within the folder\n",
        "        save_to_file(html_content, folder_path, filename + \"_html.html\")\n",
        "        save_to_file(extracted_text, folder_path, filename + \"_extracted_text.txt\")\n",
        "def main():\n",
        "    sitemap_path='resgroup__sitemap.xml'\n",
        "    urls = parse_sitemap(sitemap_path)\n",
        "    for url in urls:\n",
        "        folder_to_store=\"storage\"\n",
        "        crawl_url(url, folder_to_store)\n",
        "main()"
      ],
      "metadata": {
        "id": "mPjSiTU3KlWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}