"""intern_task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgaQ1ysZgCy_XkMXuYAHVmkysFCFhues
"""

#extracting urls from sitemap
import requests
from bs4 import BeautifulSoup

def parse_sitemap(sitemap_path):
  try:
    #using get req to get data
    f=requests.get(sitemap_path)
    k=f.content
    #using xml parser to find all links
    soup = BeautifulSoup(k, 'xml')
    urls_list = []
    #getting all the links
    loc_tags =soup.find_all('loc')
    #adding all links into a list
    for i in loc_tags:
          urls_list.append(i.get_text())
    #print("no.of urls=",len(urls_list))
    return urls_list
  except Exception as e:
        print(f"An error occurred: {e}")
        return None

import re

def clean_html_and_extract_text(html_text):
  try:
    #using a html parser
      soup=BeautifulSoup(html_text,'html.parser',from_encoding="iso-8859-1")

      for data in soup(['style', 'script']):
          # Remove tags
          data.decompose()

      #retriving only inner text of an element
      k=' '.join(soup.stripped_strings)
      #removing all special characters
      without_spcl_char=re.sub(r'[^a-zA-Z0-9\s]+', '', k)
      #removing extra whitespaces
      cleaned_text=" ".join(without_spcl_char.split())

      return cleaned_text.lower()
  except Exception as e:
    print(f"An error occurred: {e}")
    return None

#to extract content from url
import requests

def fetch_html_content(url):
  try:
    data=requests.get(url)
    return data.content
  except Exception as e:
        print('There was an error in fetching {}: {}'.format(url, e))
        return None

#to save extracted html and text content into file
import os
from os.path import exists
from bs4 import BeautifulSoup

def save_to_file(text,path,name):
  try:
    path+="/html_text"
    if not os.path.exists(folder_path):
          os.makedirs(folder_path)
    #check if file already exsists
    if exists(name):
          print('HTML file already exists')
          return


      #fun=open(os.path.join(path,name),"w")
    with open(os.path.join(path,name),"w") as file:
            file.write(str(text))
  except Exception as e:
        print(f'An error occurred while saving to file {name}: {e}')

#generating filenames for files
from pathlib import Path
from urllib.parse import urlparse, unquote

def generate_file_name(url):
    # Parse URL to get path
    path = urlparse(url).path
    # Remove special characters and decode URL encoding
    filename = unquote("".join([c if c.isalnum() or c in ('_', '-') else '_' for c in path]))
    # Limit filename length to avoid issues
    filename = filename[:255]  # Adjust the limit as needed
    return filename

#to download pdf from url
import requests
import os
from urllib.parse import urlparse
def download_pdf(url,folder_path):
        folder_path+="/pdfs"
        if not os.path.exists(folder_path):
              os.makedirs(folder_path)
        filename=generate_file_name(url)+".pdf"
        if exists(filename):
          print('file already exists')
          return

        response = requests.get(link)
        pdf = open(os.path.join(folder_path,filename), 'wb')
        pdf.write(response.content)
        pdf.close()


#crawling urls of sitemap
from bs4 import BeautifulSoup
import requests
from urllib.parse import urlparse


def crawl_url(url, folder_path):
  #to check type of url
  try:
    r = requests.get(url)
    content_type = r.headers.get('content-type')
#pdf file
    if 'application/pdf' in content_type:
      download_pdf(url, folder_path)
#html file
    #elif 'text/html' in content_type:
    else:
        filename = generate_file_name(url)
        if exists(filename):
          print('file already exists')
          return
        # Fetch the website content using requests or another library.
        html_content = fetch_html_content(url)
        soup=BeautifulSoup(html_content,"html.parser")

        # Clean the HTML content using the clean_html function.
        extracted_text = clean_html_and_extract_text(html_content)
        # Store the HTML content and extracted text in separate files within the folder.
        # here filename is the crawled_url_in_underscores

        save_to_file(soup.prettify(), folder_path, f"{filename}.html")
        save_to_file(extracted_text, folder_path, f"{filename}.txt")
    #else:
     # print('Unknown type: {}'.format(content_type))
  except Exception as e:
      print(url,e)


#main function getting sitemap_path
import os
import requests

folder_path="/content/task1"
sitemap_path="https://raw.githubusercontent.com/Acuration/acuration-data-store/main/honeywell_sitemap.xml"
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
    #to get list of links in siitemap
urls_list=parse_sitemap(sitemap_path)

for link in urls_list:
    #cleaning and extracting data
    crawl_url(link,"/content/task1")
