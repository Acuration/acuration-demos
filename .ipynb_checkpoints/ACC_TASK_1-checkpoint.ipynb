{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "218c38ba-73aa-46aa-b0f4-4f44c9a89198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import fitz\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf4c1096-583b-4b78-9203-2033b84ef059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sitemap(sitemap_path):\n",
    "    try:\n",
    "        # Parsing\n",
    "        tree = ET.parse(sitemap_path)\n",
    "        root = tree.getroot()\n",
    "        # Define the XML namespace\n",
    "        ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        # Find all URL elements in the sitemap\n",
    "        url_elements = root.findall('.//ns:url/ns:loc', namespaces=ns)\n",
    "        # Extract URLs from the elements\n",
    "        url_list = [url_element.text for url_element in url_elements]\n",
    "        return url_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing sitemap: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a6313f5-c975-4189-a455-1f5b3d7a13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_and_extract_text(html_text):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text_content = soup.get_text(separator=' ', strip=True)\n",
    "        # Replace multiple spaces with a single space\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text_content)  \n",
    "        # Remove non-alphanumeric characters\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)  \n",
    "        # Normalize the text (convert to lowercase)\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with parser. Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "464be8f0-23d5-45b1-b3b2-bee1e541f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_content(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check if the content is PDF\n",
    "        if 'application/pdf' in response.headers.get('Content-Type', ''):\n",
    "            return response.content, 'pdf'\n",
    "        else:\n",
    "            return response.text, 'html'\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch content from {url}. Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c0b2cb8-812e-4a5e-8500-3b01f7853d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(content, folder_path, filename,):\n",
    "    try:\n",
    "        if content is not None:\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Write the content to the file\n",
    "            with open(file_path,'w', encoding='utf-8') as file:\n",
    "                file.write(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to file. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8bb2ca10-fb6b-49f1-8ccb-f948377b58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pdf_to_file(pdf_content, folder, filename):\n",
    "    try:\n",
    "        pdf_folder = os.path.join(folder, 'pdf_files')\n",
    "        os.makedirs(pdf_folder, exist_ok=True)  # Ensure the folder exists\n",
    "        file_path = os.path.join(pdf_folder, filename + \"_pdf.pdf\")\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(pdf_content)\n",
    "\n",
    "        # Extract text from the downloaded PDF file\n",
    "        pdf_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "        pdf_text_folder = os.path.join(folder, 'pdf_text_files')\n",
    "        os.makedirs(pdf_text_folder, exist_ok=True)  # Ensure the folder exists\n",
    "        text_file_path = os.path.join(pdf_text_folder, filename + \"_pdf_extracted_text.txt\")\n",
    "        if pdf_text:\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(pdf_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving PDF to file. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54a0180b-82cf-445c-a86a-3c3fa90ae11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_document:\n",
    "            for page_number in range(pdf_document.page_count):\n",
    "                page = pdf_document[page_number]\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF. Error: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5494c1d9-70b5-4e31-a2e2-ade857106ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_url(url, folder_path):\n",
    "    try:\n",
    "        content, content_type = fetch_content(url)\n",
    "\n",
    "        if content is not None and content_type:\n",
    "            hashed_url = hashlib.md5(url.encode()).hexdigest()\n",
    "            filename = \"crawled_\" + hashed_url + urls[i].replace(\"/\", \"_\") + \"_\"\n",
    "\n",
    "            if content_type == 'pdf':\n",
    "                save_pdf_to_file(content, folder_path, filename + \"_pdf.pdf\")\n",
    "            else:\n",
    "                # Handle HTML content as before\n",
    "                html_content = content\n",
    "                extracted_text = clean_html_and_extract_text(html_content)\n",
    "                html_folder = os.path.join(folder_path, 'html_files')\n",
    "                save_to_file(html_content, html_folder, filename + \"_html.html\")\n",
    "                text_folder = os.path.join(folder_path, 'html_text_files')\n",
    "                save_to_file(extracted_text, text_folder, filename + \"_extracted_text.txt\")\n",
    "        else:\n",
    "            hashed_url = hashlib.md5(url.encode()).hexdigest()\n",
    "            filename = \"crawled_\" + hashed_url + urls[i].replace(\"/\", \"_\") + \"_\"\n",
    "            error_folder=os.path.join(folder_path,'error_files')\n",
    "            save_to_file(url, error_folder, filename)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling URL {url}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0590df0b-931e-497b-a0e3-efe8ac7c377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        sitemap_path = 'drax_sitemap.xml'\n",
    "        urls = parse_sitemap(sitemap_path)\n",
    "        urls=set(urls)\n",
    "        urls=list(urls)\n",
    "        for i in range(1200,1350):\n",
    "            hashed_url = hashlib.md5(urls[i].encode()).hexdigest()\n",
    "            filename = \"crawled_\" + hashed_url + urls[i].replace(\"/\", \"_\") + \"_\"\n",
    "            folder_to_store = \"crawled\"\n",
    "            crawl_url(urls[i], folder_to_store)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba715c-a8cd-49d1-8f72-bb74325d7171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error crawling URL https://www.drax.com/uk/press_release/greenest-christmas-fossil-fuels-fall-to-all-time-low-on-britains-power-grid/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/carbon-capture/the-policy-needed-to-save-the-future/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/carbon-capture/transporting-carbon-how-to-safely-move-co2-from-the-atmosphere-to-permanent-storage/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/wp-content/uploads/sites/10/2021/05/8044_Drax_Opus_Broker_Privacy_Notice.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/wp-content/uploads/sites/10/2019/05/Drax_Fuel_Cells_v6-2.mp4. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/wp-content/uploads/2019/11/Fig-12.2-Local-Highway-Network.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/press_release/virtual-tours-of-britains-biggest-power-station-to-support-home-learning/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/press_release/haven-power-generated-398-million-and-supported-over-3400-jobs-in-the-east-of-england/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/wp-content/uploads/2019/12/1.1-Introduction-to-the-Applicant-and-Guide-to-the-Application-Final.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/sustainable-bioenergy/more-power-per-pound/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/press_release/worlds-biggest-carbon-removals-deal-announced-at-new-york-climate-week/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/tag/climate-change/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/wp-content/uploads/2019/11/Fig-6.6-Contour-Plot-of-Maximum-Rolling-8-Hour-CO-Concentrations.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/wp-content/uploads/sites/8/2023/03/Virtual-work-experience-Covering-letter-guide97.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/sustainable-bioenergy/evaluating-regrowth-post-harvest-with-accurate-data-and-satellite-imagery/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/press_release/progress-power-planning-consent-extension/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/ca/management_profile/andrew-meyer-vp-fibre-procurement-north-america/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/press_release/scottish-planning-minister-visits-iconic-hollow-mountain-power-station/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/tag/sustainable-biomass-program-sbp/page/3/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/tag/beccs-bioenergy-carbon-capture-storage/page/3/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/sustainability/sustainable-bioenergy/the-biomass-carbon-calculator/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/press_release/drax-spokesperson-comments-on-consultative-ballot/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/resources/campaigns-and-trade-organisations/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/wp-content/uploads/sites/8/2019/11/HPL_Ecological_Mitigation.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/about-us/our-sites-and-businesses/hydro-electric-schemes/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/wp-content/uploads/sites/8/2019/11/Appendix-8.13-Preliminary-Ecological-Appraisal-2014.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/press_release/drax-to-supply-free-energy-to-local-care-homes-during-covid-19-crisis/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/wp-content/uploads/sites/8/2019/11/6.2-Environmental-Statement-Appendices-Volume-H-Geo-Part-Ib.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/wp-content/uploads/2020/11/Negative-Carbon-Studies-Rebuttal5.pdf. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/press_release/drax-invests-6m-in-galloway-hydro-scheme-refurbishment/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/power-generation/shock-absorbers-keeping-grid-stable/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/uk/press_release/drax-power-station-dco-has-been-upheld-following-high-court-judgement/. Error: 'str' object has no attribute 'netloc'\n",
      "Error crawling URL https://www.drax.com/us/wp-content/uploads/sites/10/2020/06/s13705-020-00255-4.pdf. Error: 'str' object has no attribute 'netloc'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df45ae01-babd-4ee7-bdab-c1d2760de727",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.drax.com/wp-content/uploads/2022/07/DR1500_Climate-policy_AM_V003.pdf'\n",
    "parsed_url = urlparse(url)\n",
    "filename = \"crawled_\" + parsed_url.netloc + parsed_url.path.replace(\"/\", \"_\") + \"_\"\n",
    "folder_to_store = \"crawled_data\"\n",
    "if is_file_present(folder_to_store, filename):\n",
    "    None\n",
    "else:\n",
    "    crawl_url(url, folder_to_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
